import streamlit as st
import pandas as pd
import csv
import io
from scraper import scrape_clinic_site
from preprocessor import preprocess_data
from seo_optimizer import generate_seo_proposals
import re

# ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰èªè¨¼æ©Ÿèƒ½ã‚’è¿½åŠ 
def check_password():
    """Returns `True` if the user had the correct password."""

    def password_entered():
        """Checks whether a password entered by the user is correct."""
        if st.session_state["password"] == st.secrets["password"]:
            st.session_state["password_correct"] = True
            del st.session_state["password"]  # don't store password
        else:
            st.session_state["password_correct"] = False

    if "password_correct" not in st.session_state:
        # First run, show input for password.
        st.text_input(
            "ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„", type="password", on_change=password_entered, key="password"
        )
        return False
    elif not st.session_state["password_correct"]:
        # Password not correct, show input + error.
        st.text_input(
            "ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„", type="password", on_change=password_entered, key="password"
        )
        st.error("ğŸ˜• ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰ãŒæ­£ã—ãã‚ã‚Šã¾ã›ã‚“")
        return False
    else:
        # Password correct.
        return True

def main():
    st.title("ã‚¯ãƒªãƒ‹ãƒƒã‚¯SEOæœ€é©åŒ–æ”¯æ´ãƒ„ãƒ¼ãƒ«")
    st.markdown("âœ¨æ›´æ–°æƒ…å ±  \nâ–¼ver1.0.0  \nãƒ„ãƒ¼ãƒ«ä½œæˆã—ã¾ã—ãŸã€‚")
    st.markdown("<font color='red' style='font-size: 13px;line-height:1.5;'>URLã«blogãŒå…¥ã£ã¦ã„ã‚‹å ´åˆã¯æ¤œç´¢å¯¾è±¡ã«ãªã‚Šã¾ã›ã‚“ï¼ˆè†¨å¤§ãªé‡ã«ãªã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ãŸã‚ï¼‰ã€‚<br>ãã®ãŸã‚ã€ã‚‚ã—ã‚‚è¨ºç™‚æ¡ˆå†…ã¨ã—ã¦blogã‚’ä½¿ç”¨ã•ã‚Œã¦ã„ã‚‹åŒ»é™¢ãŒã‚ã‚Šã¾ã—ãŸã‚‰ã€<br>ãŠæ‰‹æ•°ã§ã™ãŒã€æ‰‹å‹•ã«ã¦ã”ç¢ºèªã‚’ãŠé¡˜ã„ã„ãŸã—ã¾ã™ã€‚</font>", unsafe_allow_html=True)
    st.markdown("â€»å¿…ãšã€äººã®ç¢ºèªã‚’ã„ã‚Œã¦ãã ã•ã„ã€‚  \nâ€»GPT4o-miniã‚’ä½¿ç”¨ã—ã¦ã„ã¾ã™ã€‚å€‹äººæƒ…å ±ã®å…¥åŠ›ã¯ã”é æ…®ãã ã•ã„ã€‚  \nâ€»å¤–éƒ¨LLMã‚’ä½¿ç”¨ã—ã¦ã„ã‚‹ãŸã‚æ–™é‡‘ãŒç™ºç”Ÿã—ã¾ã™ã€‚è¨ˆç”»çš„ã«ãŠä½¿ã„ãã ã•ã„ã€‚")
    
    # ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚¹ãƒ†ãƒ¼ãƒˆã®åˆæœŸåŒ–
    if 'seo_proposals' not in st.session_state:
        st.session_state.seo_proposals = None
    if 'analysis_complete' not in st.session_state:
        st.session_state.analysis_complete = False
    if 'clinic_name' not in st.session_state:
        st.session_state.clinic_name = ""

    # ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›
    clinic_url = st.text_input("ã‚¯ãƒªãƒ‹ãƒƒã‚¯ã‚µã‚¤ãƒˆã®URLã‚’å…¥åŠ›ã—ã¦ãã ã•ã„")
    seo_goal = st.text_area("SEOæœ€é©åŒ–ã®ç›®æ¨™ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„")

    if st.button("åˆ†æé–‹å§‹"):
        if clinic_url and seo_goal:
            try:
                # é€²æ—ãƒãƒ¼ã®åˆæœŸåŒ–
                progress_bar = st.progress(0)
                status_text = st.empty()

                # ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°
                scraped_data = scrape_clinic_site(clinic_url, lambda p, m: update_progress(progress_bar, status_text, p, m))
                
                # ã‚¯ãƒªãƒ‹ãƒƒã‚¯åã®æŠ½å‡º
                st.session_state.clinic_name = extract_clinic_name(scraped_data)
                
                # ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†
                update_progress(progress_bar, status_text, 0.7, "ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†ä¸­...")
                processed_data = preprocess_data(scraped_data)
                
                # SEOæœ€é©åŒ–ææ¡ˆç”Ÿæˆ
                update_progress(progress_bar, status_text, 0.8, "SEOæœ€é©åŒ–ææ¡ˆç”Ÿæˆä¸­(GPT-4o-mini)...")
                st.session_state.seo_proposals = generate_seo_proposals(processed_data, seo_goal)
                
                # åˆ†æå®Œäº†ãƒ•ãƒ©ã‚°ã‚’è¨­å®š
                st.session_state.analysis_complete = True
                
                # çµæœè¡¨ç¤º
                update_progress(progress_bar, status_text, 1.0, "å®Œäº†")

            except Exception as e:
                st.error(f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
        else:
            st.error("URLã¨SEOç›®æ¨™ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")

    # çµæœã®è¡¨ç¤ºï¼ˆã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚¹ãƒ†ãƒ¼ãƒˆã‚’ä½¿ç”¨ï¼‰
    if st.session_state.analysis_complete:
        display_results(st.session_state.seo_proposals)

        # CSVãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒœã‚¿ãƒ³ã®è¿½åŠ 
        csv = convert_to_csv(st.session_state.seo_proposals)
        file_name = f"{st.session_state.clinic_name}_SEOæœ€é©åŒ–æ¡ˆ.csv"
        st.download_button(
            label="çµæœã‚’CSVã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
            data=csv.encode('utf-8-sig'),  # BOMã‚’è¿½åŠ ã—ã¦Excelã§é–‹ã„ãŸã¨ãã«æ–‡å­—åŒ–ã‘ã—ãªã„ã‚ˆã†ã«ã™ã‚‹
            file_name=file_name,
            mime="text/csv",
        )

def update_progress(progress_bar, status_text, progress, message):
    progress_bar.progress(progress)
    status_text.text(message)

def display_results(seo_proposals):
    st.subheader("SEOæœ€é©åŒ–ææ¡ˆ")
    for page, proposals in seo_proposals.items():
        with st.expander(f"ãƒšãƒ¼ã‚¸: {page}"):
            st.markdown("### ç¾åœ¨ã®æƒ…å ±:", unsafe_allow_html=True)
            st.write(f"ã‚¿ã‚¤ãƒˆãƒ«: {proposals['current_title']} (æ–‡å­—æ•°: {len(proposals['current_title'])})")
            st.write(f"ãƒ‡ã‚£ã‚¹ã‚¯ãƒªãƒ—ã‚·ãƒ§ãƒ³: {proposals['current_description']} (æ–‡å­—æ•°: {len(proposals['current_description'])})")
            
            st.markdown("### æœ€é©åŒ–ææ¡ˆ:", unsafe_allow_html=True)
            for i, title in enumerate(proposals['proposed_titles'], 1):
                st.write(f"ã‚¿ã‚¤ãƒˆãƒ«æ¡ˆ {i}: {title} (æ–‡å­—æ•°: {len(title)})")
            for i, desc in enumerate(proposals['proposed_descriptions'], 1):
                st.write(f"ãƒ‡ã‚£ã‚¹ã‚¯ãƒªãƒ—ã‚·ãƒ§ãƒ³æ¡ˆ {i}: {desc} (æ–‡å­—æ•°: {len(desc)})")

def convert_to_csv(seo_proposals):
    output = io.StringIO()
    writer = csv.writer(output)
    line_num = 1

    for page, proposals in seo_proposals.items():
        writer.writerow(['ãƒšãƒ¼ã‚¸URL', page, 'æ–‡å­—æ•°'])
        line_num += 1
        writer.writerow(['ç¾åœ¨ã®ã‚¿ã‚¤ãƒˆãƒ«', proposals.get('current_title', ''), f'=LEN(B{line_num})'])
        line_num += 1
        
        for i, title in enumerate(proposals.get('proposed_titles', []), 1):
            writer.writerow([f'ã‚¿ã‚¤ãƒˆãƒ«æ¡ˆ{i}', title, f'=LEN(B{line_num})'])
            line_num += 1
        
        writer.writerow(['ç¾åœ¨ã®ãƒ‡ã‚£ã‚¹ã‚¯ãƒªãƒ—ã‚·ãƒ§ãƒ³', proposals.get('current_description', ''), f'=LEN(B{line_num})'])
        line_num += 1
        
        for i, desc in enumerate(proposals.get('proposed_descriptions', []), 1):
            writer.writerow([f'ãƒ‡ã‚£ã‚¹ã‚¯ãƒªãƒ—ã‚·ãƒ§ãƒ³æ¡ˆ{i}', desc, f'=LEN(B{line_num})'])
            line_num += 1
        
        writer.writerow([])  # ç©ºè¡Œã‚’æŒ¿å…¥ã—ã¦å„ãƒšãƒ¼ã‚¸ã‚’åŒºåˆ‡ã‚‹
        line_num += 1

    return output.getvalue()

def extract_clinic_name(scraped_data):
    # ãƒˆãƒƒãƒ—ãƒšãƒ¼ã‚¸ã®ã‚¿ã‚¤ãƒˆãƒ«ã‹ã‚‰ã‚¯ãƒªãƒ‹ãƒƒã‚¯åã‚’æŠ½å‡º
    for url, data in scraped_data.items():
        if url.endswith('/') or url.endswith('/index.html'):
            title = data['title']
            # ã‚¯ãƒªãƒ‹ãƒƒã‚¯åã‚’æŠ½å‡ºã™ã‚‹æ­£è¦è¡¨ç¾ãƒ‘ã‚¿ãƒ¼ãƒ³
            match = re.search(r'(.+?)(ã‚¯ãƒªãƒ‹ãƒƒã‚¯|ç—…é™¢|åŒ»é™¢)', title)
            if match:
                return match.group(0)
    return "ã‚¯ãƒªãƒ‹ãƒƒã‚¯"  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå

if __name__ == "__main__":
    if check_password():
        main()